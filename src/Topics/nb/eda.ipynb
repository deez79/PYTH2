{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mottaquikarim/PYTH2/blob/master/src/Topics/nb/eda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Analysis II\n",
    "\n",
    "In this lesson, we'll continue exploring Pandas for EDA. Specifically: \n",
    "\n",
    "- Identify and handle missing values with Pandas.\n",
    "- Implement groupby statements for specific segmented analysis.\n",
    "- Use apply functions to clean data with Pandas.\n",
    "\n",
    "## Data sets\n",
    "\n",
    "* **[Adventureworks Cycles | Local](https://raw.githubusercontent.com/mottaquikarim/PythonProgramming/master/raw_data/production.product.tsv)**\n",
    "\t* *You can download a version of the Adventureworks Cycles dataset directly from this Github Repo* \n",
    "\n",
    "* **[OMDB Movies | Local](https://raw.githubusercontent.com/mottaquikarim/PythonProgramming/master/raw_data/movies_rated.csv)**\n",
    "\t* *You can download a version of the Adventureworks Cycles dataset directly from this Github Repo* \n",
    "\n",
    "## Let's continue with the AdventureWorks Cycles Dataset\n",
    "\n",
    "Here's the Production.Product table [data dictionary](https://www.sqldatadictionary.com/AdventureWorks2014/Production.Product.html), which is a description of the fields (columns) in the table (the .csv file we will import below):<br>\n",
    "\n",
    "- **ProductID** - Primary key for Product records.\n",
    "- **Name** - Name of the product.\n",
    "- **ProductNumber** - Unique product identification number.\n",
    "- **MakeFlag** - 0 = Product is purchased, 1 = Product is manufactured in-house.\n",
    "- **FinishedGoodsFlag** - 0 = Product is not a salable item. 1 = Product is salable.\n",
    "- **Color** - Product color.\n",
    "- **SafetyStockLevel** - Minimum inventory quantity.\n",
    "- **ReorderPoint** - Inventory level that triggers a purchase order or work order.\n",
    "- **StandardCost** - Standard cost of the product.\n",
    "- **ListPrice** - Selling price.\n",
    "- **Size** - Product size.\n",
    "- **SizeUnitMeasureCode** - Unit of measure for the Size column.\n",
    "- **WeightUnitMeasureCode** - Unit of measure for the Weight column.\n",
    "- **DaysToManufacture** - Number of days required to manufacture the product.\n",
    "- **ProductLine** - R = Road, M = Mountain, T = Touring, S = Standard\n",
    "- **Class** - H = High, M = Medium, L = Low\n",
    "- **Style** - W = Womens, M = Mens, U = Universal\n",
    "- **ProductSubcategoryID** - Product is a member of this product subcategory. Foreign key to ProductSubCategory.ProductSubCategoryID.\n",
    "- **ProductModelID** - Product is a member of this product model. Foreign key to ProductModel.ProductModelID.\n",
    "- **SellStartDate** - Date the product was available for sale.\n",
    "- **SellEndDate** - Date the product was no longer available for sale.\n",
    "- **DiscontinuedDate** - Date the product was discontinued.\n",
    "- **rowguid** - ROWGUIDCOL number uniquely identifying the record. Used to support a merge replication sample.\n",
    "- **ModifiedDate** - Date and time the record was last updated.\n",
    "\n",
    "## Loading the Data\n",
    "\n",
    "We can load our data as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "prod = pd.read_csv('/raw_data/production.product.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note the `sep='\\t'`; this is because we are pulling in a `tsv` file, which is basically a csv file but with `tabs` as delimiters vs commas.\n",
    "\n",
    "**YOU DO**: Download the `tsv` file into your local machine, create a python virtualenv and run the code above, but on your machine.\n",
    "\n",
    "\n",
    "## Handling missing data\n",
    "\n",
    "Recall missing data is a systemic, challenging problem for data scientists. Imagine conducting a poll, but some of the data gets lost, or you run out of budget and can't complete it! ðŸ˜®<br><br>\n",
    "\n",
    "\"Handling missing data\" itself is a broad topic. We'll focus on two components:\n",
    "\n",
    "- Using Pandas to identify we have missing data\n",
    "- Strategies to fill in missing data (known in the business as `imputing`)\n",
    "- Filling in missing data with Pandas\n",
    "\n",
    "## Identifying missing data\n",
    "\n",
    "Before *handling*, we must identify we're missing data at all!\n",
    "\n",
    "We have a few ways to explore missing data, and they are reminiscient of our Boolean filters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True when data isn't missing\n",
    "prod.notnull().head(3)\n",
    "# True when data is missing\n",
    "prod.isnull().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "**OUTPUT**: `notnull`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ProductID  Name  ProductNumber  MakeFlag  FinishedGoodsFlag  Color  ...  ProductModelID  SellStartDate  SellEndDate  DiscontinuedDate  rowguid  ModifiedDate\n",
    "0       True  True           True      True               True  False  ...           False           True        False             False     True          True\n",
    "1       True  True           True      True               True  False  ...           False           True        False             False     True          True\n",
    "2       True  True           True      True               True  False  ...           False           True        False             False     True          True\n",
    "\n",
    "[3 rows x 25 columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "**OUTPUT**: `isnull`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ProductID   Name  ProductNumber  MakeFlag  FinishedGoodsFlag  Color  ...  ProductModelID  SellStartDate  SellEndDate  DiscontinuedDate  rowguid  ModifiedDate\n",
    "0      False  False          False     False              False   True  ...            True          False         True              True    False         False\n",
    "1      False  False          False     False              False   True  ...            True          False         True              True    False         False\n",
    "2      False  False          False     False              False   True  ...            True          False         True              True    False         False\n",
    "\n",
    "[3 rows x 25 columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **YOU DO**: count the number of nulls in `Name` column\n",
    "* **YOU DO**: count the number of notnulls in `Name` column\n",
    "\n",
    "We can also access missing data in aggregate, as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is a quick and dirty way to do it\n",
    "prod.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name                       0\n",
    "ProductNumber              0\n",
    "MakeFlag                   0\n",
    "FinishedGoodsFlag          0\n",
    "Color                    248\n",
    "SafetyStockLevel           0\n",
    "ReorderPoint               0\n",
    "StandardCost               0\n",
    "ListPrice                  0\n",
    "Size                     293\n",
    "SizeUnitMeasureCode      328\n",
    "WeightUnitMeasureCode    299\n",
    "Weight                   299\n",
    "DaysToManufacture          0\n",
    "ProductLine              226\n",
    "Class                    257\n",
    "Style                    293\n",
    "ProductSubcategoryID     209\n",
    "ProductModelID           209\n",
    "SellStartDate              0\n",
    "SellEndDate              406\n",
    "DiscontinuedDate         504\n",
    "rowguid                    0\n",
    "ModifiedDate               0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **YOU DO**: Wrap the result from above, but into a dataframe. Sort the dataframe by column with the column with most missing data to column on top and the column with least amount of missing data on bottom.\n",
    "\n",
    "## Filling in missing data\n",
    "\n",
    "How we fill in data depends largely on why it is missing (types of missingness) and what sampling we have available to us.\n",
    "\n",
    "We may:\n",
    "\n",
    "- Delete missing data altogether\n",
    "- Fill in missing data with:\n",
    "    - The average of the column\n",
    "    - The median of the column\n",
    "    - A predicted amount based on other factors\n",
    "- Collect more data:\n",
    "    - Resample the population\n",
    "    - Followup with the authority providing data that is missing\n",
    "\n",
    "In our case, let's focus on handling missing values in `Color`. Let's get a count of the unique values in that column. We will need to use the `dropna=False` kwarg, otherwise the `pd.Series.value_counts()` method will not count `NaN` (null) values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod['Color'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NaN             248\n",
    "Black            93\n",
    "Silver           43\n",
    "Red              38\n",
    "Yellow           36\n",
    "Blue             26\n",
    "Multi             8\n",
    "Silver/Black      7\n",
    "White             4\n",
    "Grey              1\n",
    "Name: Color, dtype: int64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**We have 248 null values for `Colors`!**\n",
    "\n",
    "## Deleting missing data\n",
    "\n",
    "To delete the null values, we can:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.dropna(subset=['Color']).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will remove all `NaN` values in the color column\n",
    "\n",
    "## Filling in missing data\n",
    "\n",
    "We can fill in the missing data with a sensible default, for instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.fillna(value={'Color': 'NoColor'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This will swap all `NaN` values in `Color` column with `NoColor`.\n",
    "\n",
    "We can swap the `Color` column's null values with essentially anything we want - for instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.fillna(value={'Color': prod['ListPrice'].mean() })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **YOU DO**: Run the code above. What will it do? Does it make sense for this column? Why or why not?\n",
    "\n",
    "## Breather / Practice\n",
    "\n",
    "* **YOU DO**: Copy the `prod` dataframe, call it `prod_productline_sanitized`\n",
    "* **YOU DO**: In `prod_productline_sanitized` drop all NA values from the `ProductLine` column, **inplace**\n",
    "* **YOU DO**: Copy the `prod` dataframe, call it `prod_productline_sanitized2`\n",
    "* **YOU DO**: In `prod_productline_sanitized2`, fill all NA values with boolean `False`\n",
    "\n",
    "## Groupby Statements\n",
    "\n",
    "In Pandas, groupby statements are similar to pivot tables in that they allow us to segment our population to a specific subset.\n",
    "\n",
    "For example, if we want to know the average number of bottles sold and pack sizes per city, a groupby statement would make this task much more straightforward.\n",
    "\n",
    "\n",
    "To think how a groupby statement works, think about it like this:\n",
    "\n",
    "- **Split:** Separate our DataFrame by a specific attribute, for example, group by `Color`\n",
    "- **Combine:** Put our DataFrame back together and return some _aggregated_ metric, such as the `sum`, `count`, or `max`.\n",
    "\n",
    "![](http://i.imgur.com/yjNkiwL.png)\n",
    "\n",
    "Let's group by `Color`, and get a count of products for each color.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.groupby('Color')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how this doesn't actually *do* anything - or at least, does not print anything.\n",
    "\n",
    "Things get more interesting when we start using methods such as `count`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.groupby('Color').count().head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that count will always return non-null values, and the only way to force `groupby().count()` to ack null values is to fill nulls with `fillna` or something to that effect.\n",
    "\n",
    "Let's do something a tad more interesting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod[['Color', 'ListPrice']].groupby('Color').max().sort_values('ListPrice', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **YOU DO**: Run this code in your machine. What does it do? \n",
    "* **YOU DO**: instead of `max`, find the `min` `ListPrice` by `Color`\n",
    "* **YOU DO**: instead of `min`, find the `mean` `ListPrice` by `Color`\n",
    "* **YOU DO**: instead of `mean`, find the `count` of `ListPrice` by `Color`\n",
    "\n",
    "We can also do multi-level groupbys. This is referred to as a `Multiindex` dataframe. Here, we can see the following fields in a nested group by, with a count of Name (with nulls filled!); effectively giving us a count of the number of products for every unique Class/Style combination:\n",
    "\n",
    "- Class - H = High, M = Medium, L = Low\n",
    "- Style - W = Womens, M = Mens, U = Universal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.fillna(value={'Name': 'x'}).groupby(by=['Class', 'Style']).count()[['Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "             Name\n",
    "Class Style\n",
    "H     U        64\n",
    "L     U        68\n",
    "M     U        22\n",
    "      W        22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **YOU DO**: groupby `MakeFlag` and `FinishedGoodsFlag` and return counts of `ListPrice`\n",
    "\n",
    "We can also use the `.agg()` method with multiple arguments, to simulate a `.describe()` method like we used before:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.groupby('Color')['ListPrice'].agg(['count', 'mean', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "              count         mean     min      max\n",
    "Color\n",
    "Black            93   725.121075    0.00  3374.99\n",
    "Blue             26   923.679231   34.99  2384.07\n",
    "Grey              1   125.000000  125.00   125.00\n",
    "Multi             8    59.865000    8.99    89.99\n",
    "Red              38  1401.950000   34.99  3578.27\n",
    "Silver           43   850.305349    0.00  3399.99\n",
    "Silver/Black      7    64.018571   40.49    80.99\n",
    "White             4     9.245000    8.99     9.50\n",
    "Yellow           36   959.091389   53.99  2384.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **YOU DO**: groupby `MakeFlag` and `FinishedGoodsFlag` and return `agg` of `ListPrice` by `['count', 'mean', 'min', 'max']`.\n",
    "* **YOU DO**: do the results from above make sense? print out the dataframe of `MakeFlag`, `FinishedGoodsFlag` and `ListPrice` to see if they do or not.\n",
    "\n",
    "## Apply Functions\n",
    "\n",
    "Apply functions allow us to perform a complex operation across an entire columns or rows highly efficiently.\n",
    "\n",
    "For example, let's say we want to change our colors from a word, to just a single letter. How would we do that?\n",
    "\n",
    "The first step is writing a function, with the argument being the value we would receive from each cell in the column. This function will mutate the input, and return the result. This result will then be _applied_ to the source dataframe (if desired).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_to_letter(col):\n",
    "    if  pd.isna(col['Color']):\n",
    "        return 'N'\n",
    "\n",
    "    return col['Color'][0].upper()\n",
    "\n",
    "prod[['Color']].apply(color_to_letter, axis=1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0    N\n",
    "1    N\n",
    "2    N\n",
    "3    N\n",
    "4    N\n",
    "5    B\n",
    "6    B\n",
    "7    B\n",
    "8    S\n",
    "9    S\n",
    "Name: Color, dtype: object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `axis=1` refers to a **row** operation. Consider the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[4, 9],] * 3, columns=['A', 'B'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   A  B\n",
    "0  4  9\n",
    "1  4  9\n",
    "2  4  9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using `apply` functions, we can do:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "which would give us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     A    B\n",
    "0  2.0  3.0\n",
    "1  2.0  3.0\n",
    "2  2.0  3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can also apply to either `axis`, `1` for **rows** and `0` for **columns**.\n",
    "\n",
    "* **YOU DO**: using `np.sum` as apply function, run along rows of df above.\n",
    "* **YOU DO**: using `np.sum` as apply function, run along columns of df above.\n",
    "\n",
    "## Wrap up\n",
    "\n",
    "We've covered even more useful information! Here are the key takeaways:\n",
    "\n",
    "- **Missing data** comes in many shapes and sizes. Before deciding how to handle it, we identify it exists. We then derive how the missingness is affecting our dataset, and make a determination about how to fill in values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pro tip for identifying missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Grouby** statements are particularly useful for a subsection-of-interest analysis. Specifically, zooming in on one condition, and determining relevant statstics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by \n",
    "df.groupby('column').agg['count', 'mean', 'max', 'min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Apply functions** help us clean values across an entire DataFrame column. They are *like* a for loop for cleaning, but many times more efficient. They follow a common pattern:\n",
    "1. Write a function that works on a single value\n",
    "2. Test that function on a single value\n",
    "3. Apply that function to a whole column\n",
    "\n",
    "## OMDB Movies\n",
    "\n",
    "1. **Import the data CSV as dataframe** *(See above for link to dataset)*\n",
    "2. **Print first 5 rows**\n",
    "3. **Print out the num rows and cols in the dataset**\n",
    "4. **Print out column names**\n",
    "5. **Print out the column data types**\n",
    "6. **How many unique genres are available in the dataset?**\n",
    "7. **How many movies are available per genre?**\n",
    "8. **What are the top 5 R-rated movies?** *(hint: Boolean filters needed! Then sorting!)*\n",
    "9. **What is the average Rotten Tomatoes score for all available films?**\n",
    "10. **Same question as above, but for the top 5 films**\n",
    "11. **What is the Five Number Summary like for top rated films as per IMDB?**\n",
    "12. **Find the ratio between Rotten Tomato rating vs IMDB rating for all films. Update the dataframe to include a `Ratings Ratio` column *(inplace)***.\n",
    "13. **Find the top 3 ratings ratio movies (rated higher on IMBD compared to Rotten Tomatoes)**\n",
    "14. **Find the top 3 ratings ratio movies (rated higher on IMBD compared to Rotten Tomatoes)**\n",
    "\n",
    "\n",
    "## Pandas Reference\n",
    "\n",
    "\n",
    "At a high-level, this section will will cover:\n",
    "\n",
    "* [Joining & Concatenating](eda.md#joining--concatenating)\n",
    "* [Grouping](eda.md#grouping)\n",
    "* [Filtering](eda.md#filtering)\n",
    "* [Descriptive Statistics](eda.md#descriptive-statistics)\n",
    "\n",
    "\n",
    "## Joining & Concatenating\n",
    "\n",
    "* `df1.append(df2)` -- add the rows in df1 to the end of df2 (columns should be identical)\n",
    "* `df.concat([df1, df2],axis=1)`â€Šâ€”-â€Šadd the columns in df1 to the end of df2 (rows should be identical)\n",
    "* `df1.join(df2,on=col1,how='inner')`â€Šâ€”-â€ŠSQL-style join the columns in df1 with the columns on df2 where the rows for col have identical values. how can be equal to one of: 'left', 'right', 'outer', 'inner'\n",
    "* `df.merge()` -- merge two datasets together into one by aligning the rows from each based on common attributes or columns. how can be equal to one of: 'left', 'right', 'outer', 'inner'\n",
    "\n",
    "## Reshaping\n",
    "\n",
    "* `df.transform(func[, axis])` -- return DataFrame with transformed values\n",
    "* `df.transpose(*args, **kwargs)` -- transpose rows and columns\n",
    "* `df.rank()` -- rank every variable according to its value\n",
    "* `pd.melt(df)` -- gathers columns into rows\n",
    "* `df.pivot(columns='var', values='val')` -- spreads rows into columns\n",
    "\n",
    "## Grouping w. GroupBy Objects\n",
    "\n",
    "* `df.groupby(col)` -- returns groupby object for values from a single, specific column\n",
    "* `df.groupby([col1,col2])` -- returns a groupby object for values from multiple columns, which you can specify\n",
    "\n",
    "## Filtering\n",
    "\n",
    "\n",
    "## Descriptive Statistics\n",
    "\n",
    "* `df[col1].unique()` -- returns an ndarray of the distinct values within a given series\n",
    "* `df[col1].nunique()` -- return # of unique values within a column\n",
    "* `.value_counts()` -- returns count of each unique value\n",
    "* `df.sample(frac = 0.5)` - randomly select a fraction of rows of a DataFrame\n",
    "* `df.sample(n=10)` - randomly select n rows of a DataFrame\n",
    "* `mean()` -- mean\n",
    "* `median()` -- median\n",
    "* `min()` -- minimum\n",
    "* `max()` -- maximum\n",
    "* `quantile(x)` -- quantile\n",
    "* `var()` -- variance\n",
    "* `std()` -- standard deviation\n",
    "* `mad()` -- mean absolute variation\n",
    "* `skew()` -- skewness of distribution\n",
    "* `sem()` -- unbiased standard error of the mean\n",
    "* `kurt()` -- kurtosis\n",
    "* `cov()` -- covariance\n",
    "* `corr()` -- Pearson Correlation coefficent\n",
    "* `autocorr()` -- autocorelation\n",
    "* `diff()` -- first discrete difference\n",
    "* `cumsum()` -- cummulative sum\n",
    "* `comprod()` -- cumulative product\n",
    "* `cummin()` -- cumulative minimum\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
